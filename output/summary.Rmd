---
title: "From LATE to GREAT"
subtitle: "Model-based extrapolation of local average treatment effects"
author: "Brice Green"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: biblio.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

# Abstract

Causal estimates are critical for understanding policy interventions, but without assuming that treatment effects are constant it is difficult to know how to extrapolate those estimates to new populations. This paper proposes a framework borrowed from the survey literature for directly measuring how treatment effects vary across the population, and demonstrates how to use the model to extrapolate to new populations. 

# Introduction

Accurate inference for causal parameters is critical to testing theories and designing policy interventions. However, in a social science context, it is often implausible that all subjects respond the same way to interventions. While we can (and regularly do) relax the assumption of constant treatment effects, we are left with an estimate that is permanently glued to our specific sample.

Our hope as scientists and policy makers is to be able to make more general statements, projecting earlier estimates onto new samples, either to uphold/debunk theories or to use evidence to influence policy trade-offs. The ability to extrapolate parameters from previous estimates is key to tying causal estimation to future policy.

One great advantage of modern measurement is that we have rich information about the different subjects in our studies, and even use that information to adjust our estimates for factors that we could not adequately randomize on or to accommodate different baselines from which we are measuring treatment responses. I propose using that information not only to adjust estimates of the average, but also to directly estimate how treatment effects vary across the population, estimating how model parameters vary with observable characteristics of the subjects.

Through the paper I am going to be dealing with the estimation of a treatment effect given either an experiment with non-compliance or a "natural" experiment, presuming that the analyst does not know a priori that treatment effects are constant. The treatment effects, in this case, are a kind of latent response surface, varying across characteristics that are either observed or not, corresponding to the expected outcome for an individual receiving treatment. I will work with binary treatments and continuous outcomes in an instrumental variables setting, though the results generalize easily to an experimental context with full compliance. [^1]

[^1]: It is less obvious how this would generalize to a regression discontinuity design or similar inference methods.

Given subjects indexed by $i$, a binary treatment $d \in \{0,1\}$, a continuous or binary instrumental variable $Z$, and an outcome $Y$, let $Y_i(1)$ denote the outcome for individual $i$ had they gotten the treatment and $Y_i(0)$ indicate the outcome had they not recieved the treatment. 

$$\tau = E(Y_i(1)) - E(Y_i(0))$$

In general without full compliance, 

$$\tau \neq E(Y_i(1) - Y_i(0)).$$

In economics, we commonly expect this to be violated, as people making choices tend to choose things which they believe will bring them outcomes that they prefer, inducing a strong correlation between outcomes and treated status. To solve this, we introduce an instrumental variable $Z$, such that $d$ is a non-trivial function of $Z$.

Following the standard literature, I call "compliers" those in the sample who would have taken the treatment, given a shock from $Z$, but would not have otherwise. This is in comparison to "always-takers" who would take the treatment regardless of $Z$, and "never-takers" are those who can never be induced by $Z$ to take up treatment. Assuming non-interference ($d_i$ depends on $z_i$ and not $z_{j \neq i}$)  excludability (that outcomes do not depend on $z_i$ except via $d_i$, and monotonicity (an increase in $Z$ never leads to a _decrease_ in the probability of treatment uptake).

Furthermore, I presume that everything of interest has been measured and incorporated into the model, or that given observable characteristics $X$, we expect treatment effects $\tau$ to be a function of $X$, and not variables we have not measured. 

In this context, I propose a four-step plan from moving from LATE to Great (no acronym as of now):

1. Bin
2. Model
3. Project
4. Maximize

First, we bin observations into groups, based on their observable characteristics. When variables are continuous, this involves discretizing the variable into bins. Second we create a principled model that describes how the cells relate to one another. Third, we take our estimates from our first population and project them onto a new one based on know or expected weights for the new population. The fourth step, should you need it, uses the projections of the model in combination with a utility (or loss) function that represents policy trade-offs to maximize (minimize) expected utility for a new policy roll-out.

The most novel parts of this paper fall under steps 2 and 4, as it is already current practice to perform "Stratified IV" in order to analyze how treatment effects vary across groups. However, as the number of dimensions grows, for a fixed number of bins per variable, the number of observations to identify effects in a given bin we need grows exponentially. This curse of dimensionality necessitates some type of approach.

In the machine learning literature it is common to construct estimators that induce sparsity so as to remove variables that are not highly predictive. However, in this context sparsity may not be desirable; typically we think of treatment effects as being smooth, and simply setting the marginal movement of a bin around a central value to zero constructs a jagged set of point estimates. In addition, we are often interested in uncertainty around treatment effects, and quantifying that estimation uncertainty is non-trivial for penalized estimators. Finally, depending on the size of the grid, we may not expect the marginal effect of a given bin to be large, but we may expect them to be non-zero. It may actually be preferable to estimate a series of small, smoother changes across a large number of points in a grid, rather than to construct a grid that is wider or smaller depending on point predictions.

I propose a different type of model that relates estimates across cells through a latent likelihood equation. In other words, given some grid-wise sampling density

$$Y_i \sim f(\tau_i, \theta),$$

I construct a model that describes the variation of the treatment effects

$$\tau_i \sim g(\theta),$$
and finally I put a prior on the hyper-parameters $\theta$ that penalize the complexity of the model to prevent overfitting. So, finally, 

$\theta \sim \Pi(\theta).$

This is commonly referred to a "multilevel" or "hierarchical" model, referring to its three-level structure, though it can be collapsed into a single likelihood. The structure itself is quite flexible, as the likelihood function $f$ can correspond to whatever the appropriate likelihood is for the observed data. Typically $g$ is parameterized by a normal distribution, owing to the central limit theorem, with a mean linearized out and variation around that mean parameterized depending on the specific application.

The goal of $g$ and $\Pi$ in this structure is not to provide strong information about the specific realization we expect (say, incorporating information from a previous study about the treatment effect itself), but instead to describe a class of models that support the local regularities we believe structurally exist in the data generating process. All the while, it would be unwarranted to construct $g$ in such as way as to make believable parameter values unlikely or rule out possibilities altogether. 

The trick to why this model works so effectively is that it no longer forces the various cells to be estimated independently of each other, and in the limit can accommodate both complete dependence and complete independence between cell-wise means. As the number of parameters to be estimated approach or exceed the number of observations we have available, it can limit the search for the appropriate model to a reasonable class.

But what counts as reasonable likely depends on the context. In this paper, I propose two a multilevel model that can approximate arbitrary variation of treatment effects along observed continuous, ordinal, and unordered variables, demonstrating them to be good starting points for estimating treatment effects and projecting them onto new populations. 

Specifically, for continuous and ordinal variables, I treat the covariate lattice as a Gaussian Markov Random Field. In two dimensions, call $\theta_{i,j}$ a parameter in a cell along an even grid of binned covariates. Its neighbors are then $\Gamma = \{\theta_{i-1, j}$, $\theta_{i+1, j}$, $\theta_{i, j-1}\}$, and $\theta_{i, j+1}$. The simplifying assumption of a Gaussian Markov Random Field (relative to a general random field) is that all of the information relevant to estimating cell $\theta_{i,j}$ is incorporated in its neighboring estimates. In other words, for some parameter

$$P(\theta_{i,j} | \theta_{i,-j}; \theta_{-i,j}) = P(\theta_{i,j} | \Gamma)$$

We can then approximate this relationship with a linear basis function, sometimes called a Conditional Autoregressive Process, where 

$$\theta_{i,j} \sim Normal(\alpha \Gamma, \Sigma)$$
though typically this is estimated via the precision matrix, $Q = \Sigma^{-1}$ for computational efficiency.

For unordered variables, we have no concept of "adjacency", so I propose a nested normal prior, where 

$$\theta_{i,j} \sim Normal(\theta, \sigma),$$

a common model in education research [@rubin1981estimation], meta-analysis [@meager2019understanding], and highly related models political science [@ghitza2013deep].

While at first these parametric assumptions appear restrictive, they embed within them all important cases of interest. For example, $\alpha = 0$ corresponds to complete independence between all cells and $\alpha = 1$ corresponds to constant treatment effects. For unordered variables, as $\sigma \to 0$, we end up with constant treatment effects, and as $\sigma \to \infty$ we end up with cell-wise independent means, just as we would with stratified IV.

Using a model that fully describes the data generating process also facilitates extrapolation not just of treatment effects, but any parameter of interest. For example, if we believed there to be heteroskedasticity, in principle we could estimate the scale of the residuals as a function that varied along with characteristics. 

But more importantly, in making explicit our assumptions about the world the policy decision of the analyst is highly simplified, and what was previously an opaque set of judgements can now be discussed. If we estimate the model via a "fully Bayesian" approach, where we generate the full posterior distribution for all parameters, the projected version of those estimates represents the analysts' best guess at the state space in their new location. Presuming they have convex policy preferences that can be represented with some form of von Neumann-Morgenstern utility function, the optimal policy is simply the one that maximizes the expected utility of the policy rollout, which (given a utility function and a set of posterior draws) is trivial to estimate.

Perhaps the best argument for the model is that the empirical performance is striking, having a lower mean squared error and higher $R^2$ than 2-stage-least-squares estimation even in cases with few bins. The bias of the method reduces with finer grids without a comparable reduction in variance, generating good representations even when the number of cells exceeds the number of observations.

In addition to the paper, I have made estimation software available at github.com/be-green/late-to-great/, with efficient implementations of the models coded in Stan, a probabilistic programming language that facilitates maximum likelihood, fully Bayesian Markov Chain Monte Carlo, and Maximum a Posteriori estimation via variational Bayesian methods [@carpenter2017stan]. There are interfaces to Stan in R, Stata, Julia, and Python, so these models are also estimable through those languages.

# Background

There is a long history of instrumental variables being used to address a history of experimental and psuedo-experimental conditions without full compliance, dating back to @wright1928tariff, and growing immensely in popularity since @angrist1996identification. There is simply so much work evaluating and understanding instrumental variables approaches and I am going to limit this discussion to a specific context: we know that necessary assumptions are satisfied  (e.g. exclusion, monotonicity, etc.), and we are in the just-identified case with one instrument and one endogenous variable.

There is a substantial amount of literature in more recent years focused on extrapolating estimates from one population to a new population, and analyzing to what extent local average treatment effects are portable. In the economics literature, there is a substantial amount of attention on estimating to what extent a LATE approximates an ATE, and identifying to what extent researchers should be concerned with variation in treatment effects across different parts of the population. 

There are several papers in this vein, which largely use the observable characteristics of the various sub-groups in the analysis: the always takers, never takers, and compliers. Perhaps most closely to this paper, @abadie2003semiparametric argues for using estimators that create an approximation to a full treatment effect function that varies with observable characteristics. @kowalski2016doing proposes extrapolating to new situations or programs by examining the marginal treatment effect, or the treatment effect for the person who is marginally entering treatment given an instrument. 

However, most of these papers are concerned with modeling differences in the population which takes up the treatment from the population overall (extrapolating the local average treatment effect to the average treatment effect). While the model presented in this paper can help address the same concerns as the previous literature, it can also be used to extrapolate effectively from one experimental context to a new, quite distant, population, which might not have the same structure for take-up of the treatment at all.

This is a long-standing problem in the survey literature, especially in political science, where researchers are often interested in using polls from one state to forecast outcomes in a different state or the whole country [@wang2015forecasting], or to do inference on sets of complicated interaction effects for different sub-populations [@ghitza2013deep]. Traditionally, the approach is to take a non-representative polling population, divide those populations into several sub-categories, and re-weight the outcomes to reflect the new population total (this goes back at least as far as [@deming1940least]). 

However, this approach suffers from the curse of dimensionality: while it is desirable to have a finer grid along the characteristics you are estimating, the number of estimation cells grows geometrically with each additional characteristic you include. @gelman1997poststratification demonstrate the efficacy of multilevel regression modeling in capturing that variation, providing a structure that can both capture complete independence between cells, while also allowing for constant responses across all cells along similar characteristics. 

There is a substantial amount of recent work on this problem, notably @gao2020improving, who incorporate structural information to the modeling of variation of responses along observed characteristics, and @kennedy2019know, who applies a similar approach to an experimental context.

# The approach

Given observable characteristics about our population, I propose directly measuring the how local average treatment effects vary across those characteristics. This is to say, given a continuous covariate like income or age, the analyst would break that variable into a series of even bins, measuring a LATE for those who fall in that bucket. Discrete or ordinal variables are already in buckets, so they are already how we want them. Under normal assumptions, the LATE can only identify an effect for the population of "compliers," e.g. those who are moved into the treatment by the intervention, so this is the treatment effect we will estimate here. Thus, presuming that we have a treatment effect $\tau$ which is a smooth, integrable function of observable characteristics $X$

$$\tau = f(X),$$

we take some $\epsilon$ and calculate the average $\tau$ within that bucket using a normal means (say, a randomized trial, instrumental variables, what have you). Presume that $X$ is a random variable with support over the region being sampled with density $g(x)$ over the region being sampled. Taking the one-dimensional case for now, as the number people in this region of the parameter space approaches $\infty$, 

$$\tau_\epsilon = \int_{x-\epsilon}^{x + \epsilon}f(x)g(x) dx.$$
In the limit, as $\epsilon \to 0$, $\tau_\epsilon \to \tau$. 

Given our new knowledge of the treatment effect in that cell, we can make project this local average treatment effect onto the new population by taking a weighted average of the cell-wise LATE estimates, where the weights are determined by the new population. In the survey literature this is known as _regression and post-stratification_, a model that is used to address non-representative responses in surveys, and (for instance) combine polls across states and forecast how a new state will respond (given state-level demographic information).

In principle, one could simply estimate this via 2-stage-least-squares.

# Extrapolation and Optimal Policy

Once we have estimated this model with the data we have observed or collected, extrapolation is (in principle) a fairly easy step. The posterior distribution is a fully generative data model, and so we can just take draws from the posterior predictive distribution and calculate whatever policy-relevant statistics we want. We can check the quantiles of the treatment in the new population, expected variation, the expected value of the treatment effect, or just plot kernel density estimates, spatial maps--really whatever suits the fancy of the policy office or analyst.

Choosing the optimal policy also enters a very well-known domain. The posterior predictive distribution is a full representation of our expected data generating process as we roll out a new policy, and so the thing that is left for the policy office to do is to create a utility or loss function that represents the benefits and costs of the roll-out conditional on posterior draws, and then to make the choice that maximizes expected utility.

Obviously constructing such a function is quite difficult, but the task of doing so is a tractable problem! To be more formal about it, presume that the analyst, government, or researcher has Von Neugmann-Morgenstern preferences. Presume a choice function $C$ with parameters $\theta$ that induces an outcome $A$ given a treatment effect (or vector of treatment effects $x_i$. Then their preferences over actions can be represented by a utility function 
$$EU(\theta) = \int p_i u(C(\theta, x_i)) di$$ 

Since we have a posterior distribution for our model, 

# Estimation and Simulation Results

Because the posterior distribution for the model does not have a closed form solution, I rely on discrete approximation via simulation, estimating the full posterior distribution of these models using Hamiltonian Monte Carlo with No-U-Turn-Sampling, with models coded in the _Stan_ programming language.

I consider a case with 1,000 individuals who are set to receive incentive to take up a binary treatment. The impact of treatment is a smooth function of an observable attribute. Some people take up the treatment before the incentive when the treatment effect is high enough for them in particular. As with any LATE model, this can only identify the effect of "compliers" in each cell, that is those people who only take up the treatment _because_ of the instrument (simulated incentives in this case). I demonstrate the full range of posterior paths for models attempting to match a range of functional forms, and then compare posterior means and medians to point estimates from 2 stage least squares estimates, run by strata of the variables. While 2 stage least squares will suffer power issues as the number of buckets in a single dimension increase, this is the favored approach in recent literature. See, for example, @wang2019impact.

I consider a number of smooth functional forms, starting with one dimension and then moving to more. Finally, I randomly sample from the population to construct a non-representative sample, and demonstrate the ability to extrapolate to a new one.

## Learning the treatment effect for a population

### One-dimensional performance

First, consider the case where we a series of people who are randomly distributed along some region of our variable $X$, our functions are relatively smooth, and no buckets are empty[^1]. 

[^1]: For now, I have taken the point predictions of the first stage of the instrument variables design as given, and as a consequence I am not considering the error bars associated with these estimates. This will change in a future draft of the paper, as it is important to have full posterior draws for the joint estimation to properly represent the parameter uncertainty for the various models.

To understand the performance of the model under these conditions, I break the variable $X$ into increasingly small cells, estimating both traditional stratified two-stage-least-squares, and the model with the AR(1) prior, which allows cells to inform each other via the likelihood.

Consider first the model $\tau = X^2\beta$.

```{r echo = F, message = F, warning = F}
library(data.table)
library(magrittr)
library(stringr)

files <- list.files("tbl/posterior-mean-summary/",
                    full.names = T, pattern = "tau=")

data <- lapply(files, fread)
names(data) <- str_extract(files, "[a-zA-Z0-9-\\^=]+.txt")

data <- rbindlist(data, idcol = T)
data[, `Number of Cells` := str_extract(.id, 
                                        "groups-[0-9]+") %>% 
       str_replace_all("groups-", "") %>% 
       as.numeric]
setorder(data, `Number of Cells`)
data[,.id := NULL]

knitr::kable(data, digits = 2, caption = "Point-wise summary statistics for posterior means vs. 2sls point estimates")

```

What immediately becomes clear is that the performance of the model where the cells can inform each other _does not deteriorate_ as the number of cells gets large. Even in the case where there are only 2 people per cell, the bias is small, as is the variance. By contrast, the standard deviation of the independent estimate grows, since it is controlled by $\sqrt{N}$. In almost all cases, the mean squared error of the posterior mean is small relative to the traditional independent cell approach, and it actually shrinks as the number of cells grows!

To show an illustrative example, look at the estimates for the case where there are 50 buckets of our $X$ variable.

![](img/posterior-50-buckets.png)

Compare that to the estimates where we use 100 buckets for the $X$ variable.

![](img/posterior-100-buckets.png)

In principle you might be tempted to object that you would never need bins that were so fine, but consider this; with each additional continuous variable, the number of interactions grows geometrically. When I move to the D=2 case, the number of bins, even with just 10 breakpoints, grows to 100, so the local performance in such a situation can be quite practically important.

Now consider the functional form $\tau = \sin{x}$. This one is particularly odd because the population of compliers drops off cyclically over the distribution of X.


```{r echo = F, message = F, warning = F}
library(data.table)
library(magrittr)
library(stringr)

files <- list.files("tbl/posterior-mean-summary/",
                    full.names = T, pattern = "tausinx")

data <- lapply(files, fread)
names(data) <- str_extract(files, "[a-zA-Z0-9-]+.png")

data <- rbindlist(data, idcol = T)
data[, `Number of Cells` := str_extract(.id, 
                                        "groups-[0-9]+") %>% 
       str_replace_all("groups-", "") %>% 
       as.numeric]
setorder(data, `Number of Cells`)
data[,.id := NULL]

knitr::kable(data, digits = 2, caption = "Point-wise summary statistics for posterior means vs. 2sls point estimates")

```


In future drafts I'll include other functional forms (possibly in an appendix), but for now suffice to say that the two or three I've tested all have nearly identical results. I primarily look at point estimates in this case largely because there is nothing comparable to a posterior distribution under 2-stage-least-squares. However, one of the benefits of having a fully generative posterior distribution is that I can draw a series of candidate $\tau$ values which depend on the posterior uncertainty in my hyper-parameter $\rho$. While I still don't have the structural uncertainty from the first stage of the estimate baked into the posterior (again, this will certainly change in a future draft), it still helps us see what precisely is going on in the estimates that we are averaging across.

Consider the most recent fit with 100 groups being estimated across $X$.

```{r echo = F, message = F, warning = F, cache = T, fig.cap = "100 random draws from the posterior distribution for the estimated treatment effects vs. real treatment effect"}
library(ggplot2)
fit <- readRDS(file = "binary/simulations/tausinx/100-groups-fixed-first-stage.rds")
te <- rstan::extract(fit, "tau")$tau

tau_by_group <- fread("binary/simulations/tausinx/100-groups-real-tau.csv")%>%
    .[natural_uptake == 0 & rand_assign == 1] %>%
    .[,.(value = mean(tau), N = length(tau)),
      by = c("X_group")] %>% 
  setnames("X_group", "group") %>% 
  .[,Draw := 1]

te <- as.data.table(te)

te[, Draw := .I]

te <- te[Draw %in% sample(Draw, 100)]

melt(te, id.vars = "Draw") %>% 
  .[,group := as.integer(str_extract(variable, "[0-9]+"))] %>% 
  ggplot(aes(x = group, y = value, group = Draw)) +
  geom_line(alpha = 0.5, color = "#A31F34") +
  geom_line(data = tau_by_group, color = "black", size = 1) +
  theme_minimal() +
  xlab("X variable bin") +
  ylab("Treatment Effect")

```

So some parts of this effect seem to be covered better than others by the posterior distribution of $\tau$. One reason for this might be that the prior variance was set too high given how narrow each of these groups was, which meant that the variation away from the central treatment value is not as tightly draw to neighboring observations. right now the prior is set to a unit variance, with $\beta_i \sim Normal(\beta_{i-1}, 1)$, which might be a bit weaker than we want given the number of parameters we are trying to estimate. An important takeaway from that exercise is that when a prior is formulated around a _base_ model, adding more parameters and prior information can actually serve to regularize estimates, not to blow them out of whack! Sometime weaker priors, while they seem like they make fewer assumptions, allow for variation that we really don't believe to be appropriate and thus overfit the data. Because our priors are allowing for the model to check for variations _away_ from a constant treatment effects assumption, our understanding of that base model relative to the variation in our $X$ variables should inform our model.

To give a practical example, suppose I wanted to model healthcare uptake treatment effects in a randomized or psuedo-randomized setting. I believe that uptake, given that I randomly give a person health insurance likely varies with their distance from a provider (say a clinic or hospital). Suppose I have a measurement of this distance. A prior with a unit variance in the AR(1) process might seem appropriate when my bin is a mile; that might be another 20 minutes walk for someone without a car or an extra 40 minute drive for someone living in Los Angeles, and if my healthcare uptake outcomes are scaled to have standard deviation 1, it seems reasonable to think that I should really allow for a typical deviation (e.g. 66% of the prior density) to allow for a full standard deviation change in a treatment effect. But as my bin size gets smaller, say to the level of 2 feet, that variance starts to seem fairly inappropriate. So keep this in mind when setting priors and dealing with continuous variables; as your binsize shrinks, make sure to understand how that changes the strength of the information encoded in your priors. 

### Graduating to multiple dimensions

## Multiple X Variables

TK

## Mixture of Continuous and Unordered Covariates

TK

# Empirical Example: The Oregon Health Insurance Experiment

TK

# Empirical Example: Comparing interventions in the microcredit literature

TK (maybe)

# Discussion

Thus far, we have seen that the structure in these models is promising for dealing with a key problem in causal inference: extrapolating varying treatment effects given a non-representative population. In the limit, traditional methods that do not rely on any model structure can simply not be estimated due to a lack of data, and this issue is made worse exponentially with each additional dimension included in the analysis.

Are these models the best for analyzing causal inference problems? That, I have no answer for, because it is likely to be specific to the model, data, and context of the problem being studied. Rather, I want to advocate for the more _general_ approach presented in the paper. In order to extrapolate varying treatment effects, we need to be willing to propose a structure for the way that they vary. To do this, we can construct a prior distribution that centers around a believable base model, and incorporates information into the structure of the model. Finally, we can integrate across the candidate models made available through this structure with fully Bayesian inference, draw from the posterior predictive distribution, and weight those draws based on the new population, using the information in the model to guide policy rollouts via a utility or loss function. 

At all points it is critical that the analyst checks their model, both by making sure that simulations from the model align with the observed distribution, and by checking for pathologies in the computation, which can help detect a mismatch between the model and the available data.

Put together, these steps represent a principled and complete workflow for dealing with varying treatment effects. So if these constructions do not work with your model, then by all means come up with another! The world is yours to analyze how you wish.


