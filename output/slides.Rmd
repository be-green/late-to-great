---
title: "LATE to Great"
subtitle: "Model-based extrapolation to new populations"
author: "Brice Green"
institute: "MIT"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

# Motivation: Why aren't LATEs great already?

- Local Average Treatment Effects are trapped in their population
- What do policy-makers do with LATE estimates?
- Represent an aggregated effect that is hard to interpret directly

---
# Motivation: why aren't LATEs great already?

From Athey & Imbens, "The State of Applied Econometrics: Causality and Policy Evaluation" (2017): 

> "Even when a causal study is done carefully, both in analysis and design, there is often little assurance that the causal effects are valid for populations or settings other  than  those  studied.  "

---

# 4 Step Program

1. Bin
2. Model
3. Project
4. Maximize

---
# Bin

Partition the covariate space into subpopulations

---
# Model

Construct a model for the variation in treatment effects across the population.

---
# Project

Project our estimates onto a new population

---
# Maximize

Given a policy-maker, maximize expected utility over the state-space generated by our model (which is the best guess of future outcomes conditional on the model).

---
# Background

Analyzing Treatment Effects in Subgroups:

- Hill - Bayesian Additive Regression Trees
- Athey and Imbens - Causal Trees
- Wager and Athey - Causal Forests
- Imai  and  Ratkovic - LASSO w/ Interactions

---
# Question 1:

As we construct a finer partition of the covariate space, the number of parameters to estimate grows geometrically. The variance of traditional estimators approaches infinity really fast! How can we estimate a rich set of variation without blowing up our estimates?

---
# Question 2:

Is sparsity a desirable property in causal estimation? What if our treatment effects are a smooth function of covariates? 

---
# Question 3:

How do we connect these estimates to policy?

---
# Proposal: Incorporate Information into Estimation

- We know things about distance in a covariate space
- We can incorporate that information in the form of a prior
- Methods for exploring the parameter space are efficient and statistically well understood
- Extrapolating and optimal policy is well understood: Posterior Predictive Distribution + vNM Utility

---
# Proposal: Incorporate Information into Estimation

Use a multilevel model, e.g. a latent likelihood for the way parameters relate to each other. Basic structure

Observational Model within each cell in the partition:
$$Y_i \sim f(X_i, \theta_i)$$

Likelihood ("prior") about how parameters relate across the partitions:

$$\theta_i \sim g(\Theta)$$

Prior on the hyper-parameters in order to penalize model complexity (e.g. regularization):

$$\Theta \sim \Pi(\Gamma)$$

---
# Proposal: Incorporate Information into Estimation
## Conditions

- Construct a prior that can recover any model of interest
  - Infinite variance of treatment effects
  - Constant variance of treatment effects
- Penalize complexity of the model such that ex-ante we don't expect huge parameter values
  - Regularization of hyperparameters
- Formulate models so that hyperparameters & priors are interpretable by humans
  
---
# Proposal: Incorporate Information into Estimation
## Specific Workable Proposal

- Think of given partition as a graph
- All ordered covariates are connected
- Covariates where we don't know order are disconnected
- Sidenote: this idea can co-exist with any of the other methods for partitioning the space (e.g. partitioning via forests first or the Finite Element Method as in spatial stats), but I'm going to work with even lattices for now

---
# Proposal: Incorporate Information into Estimation
## Specific Workable Proposal

- Borrow from the spatial literature: model treatment effects as varying according to a Gaussian Markov Random Field (GMRF) via Intrinsic Conditional Autoregressive Model (Random Walk prior)
- When including covariates w/o order, model variation across disconnected graphs via latent normal distribution (Besag, York, Mollie model)
- This is not the only model that might work in this framework, but I'll show that it works really nicely
- Other possibilities: gaussian process priors, non-random-walk (exact, stationary) autoregressive process, non-normal distributions for variation, probably lots of other things

---
# Heterogenous treatment effects via GMRFs
## What is a GMRF?

- Gaussian: normally distributed
- Markov: on depends on current state
- Random: varying over its dimensions stochastically
- Field: moves 1 or more dimensions

---
# Heterogenous treatment effects via GMRFs
## What is a GMRF?

What does it imply? Let's start with a 1 dimensional example, focusing on the blue dot,. Imagine we have broken a single covariate into a set of even bins like so:

```{r echo = F, message = F, warning = F}
library(ggplot2)

df <- data.frame(x = seq(0, 10, by = 1), y = rep(1, 11), color = c(rep(0, 5), 1, rep(0, 5)))

ggplot(df,
       aes(x = x, y = y, color = factor(color))) +
  geom_point(size = 2) +
  theme_void() +
  guides(color = F) +
  scale_color_manual(values = c("red", "blue"))
```

---
# Heterogenous treatment effects via GMRFs
## What is a GMRF?

The Gaussian Markov Random Field assumption implies that all of the prior observational knowledge about the blue dot is contained in its neighbors, the green dots.

```{r echo = F, message = F, warning = F}
df <- data.frame(x = seq(0, 10, by = 1), y = rep(1, 11), color = c(rep(0, 4),1, 2, 1, 
                                                                   rep(0, 4)))

ggplot(df,
       aes(x = x, y = y, color = factor(color))) +
  geom_point(size = 2) +
  theme_void() +
  guides(color = F) +
  scale_color_manual(values = c("red", "#66CD00", "blue"))
```

One way to think about this is via an autoregressive process (an AR(1) is a special case of a GMRF). You could select any number of neighbors, but I'm just using 1 in this paper. This is a simplifying assumption, but one that has a long history and strong mathematical connections to stochastic partial differential equations (see Lindgren, Rue, LindstrÃ¶m 2011).

---
# Heterogenous treatment effects via GMRFs
## What is a GMRF?

In other words, for some parameter of interest $phi$ and variance $\sigma$,

$p(\phi_{blue} | \phi_{color \neq blue}, \sigma) = p(\phi_{blue} | \phi_{green}, \sigma_i).$

```{r echo = F, message = F, warning = F}
df <- data.frame(x = seq(0, 10, by = 1), y = rep(1, 11), color = c(rep(0, 4),1, 2, 1, 
                                                                   rep(0, 4)))

ggplot(df,
       aes(x = x, y = y, color = factor(color))) +
  geom_point(size = 2) +
  theme_void() +
  guides(color = F) +
  scale_color_manual(values = c("red", "#66CD00", "blue"))
```

---
# Heterogenous treatment effects via GMRFs
## Conditional Autoregressive Processes

This model is normally an autoregressive one, parameterized by the precision (inverse of $\sigma$). So letting $i$ = blue,

$p(\phi_{i} | \phi_{j \neq i}, \sigma_i) = N(\mu_i, \sigma_i)$ where

$\mu_{blue} = \alpha\Sigma_{j \in green} w_{i,j}\phi_j$

and $w$ indicates the weight for the distance of $j$ from $i$.

---
# Heterogenous treatment effects via GMRFs
## Conditional Autoregressive Processes

This model is normally an autoregressive one, parameterized by the precision (inverse of $\sigma$). So

$p(\phi_{blue} | \phi_{color \neq blue}, \sigma_{blue}) = N(\mu_i, \sigma_{blue})$ where

$\mu_i = \alpha\Sigma_{j \in green} w_{i,j}\phi_j$

where $w$ indicates the weight for the distance of $j$ from $i$.

---
# Heterogenous treatment effects via GMRFs
## Intrinsic Conditional Autoregressive Processes

The "intrinsic" version of this model corresponds to a random walk, where $\alpha = 1$, a knife-edge case people are most familiar with in a time series context. 

For a detailed and practical exposition, refer to Morris et al. in _Spatial and spatio-temporal epidemiology_. I'm going to follow her notation.


---
# Heterogenous treatment effects via GMRFs
## Intrinsic Conditional Autoregressive Processes

Consider a lattice with $n$ cells. Let $w_{i,j} = 1$ if $i$, $j$ are neighbors, 0 otherwise. 

Let $D$ be an $n$ x $n$ diagonal matrix with $d_{i,i}$ indicating the number of neighbors a cell in the lattice has.

Let $B$ be a scaled weights matrix where the weights are normalized by the number of neighbors.

This corresponds to the assumption that _without seeing any data_, the mean of a cell is the average of its neighbors, with uncertainty decreasing with the number of neighbors it has. After some algebra,

$$\phi \sim N(0, [q(D - W)]^{-1})$$

where $q = \sigma^{-1}.$

---
# Heterogenous treatment effects via GMRFs

For many applications this seems like a sensible prior:

- We expect treatment effects to be a smooth function of covariates
- Possibility of non-linear movement in both treatment effects and outcomes
- Want to allow for potential non-stationarity of the process
- GMRF is basically a step-wise approximation via a lattice of this stuff!
- Allows us to identify models by sharing information across cells, instead of treating them as unrelated
- Models are identifiable even as the number of observations in a bin goes to 0

---
# Heterogenous treatment effects via GMRFs
## Estimation

We can estimate this model efficiently via Hamiltonian Monte Carlo, an MCMC method which gives us a good approximation the full posterior distribution for all parameters.

`Stan` is a probabilistic programming language that allows a nice declarative syntax for custom models, while implementing all this stuff on the back-end. For the code see my [github repository](www.github.com/be-green/late-to-great) in the `src` folder.

Bayesian inference for the parameters is straightforward, since the "latent" parameters are just like any other parameter in the model!

---
# Let's see some examples!

Computational experiment:

- Binary treatment
- Strong instrument
- Set of $k$ ordered covariates
- Set of $n$ groups per covariate
- This gives us $n^k$ groups total
- Treatment effect $\tau$ is a function of covariates
- Compare this model to independent estimation on the same partition

---
# k = 1, n = 3, linear function, 1000 observations
## Regular 2sls

---
# k = 1, n = 3, linear function, 1000 observations
## 2sls w/ GMRF Prior













