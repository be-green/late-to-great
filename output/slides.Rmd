---
title: "LATE to Great"
subtitle: "Model-based extrapolation to new populations"
author: "Brice Green"
institute: "MIT"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    self_contained: true
---

# Motivation: Why aren't LATEs great already?

- Local Average Treatment Effects are trapped in their population
- What do policy-makers do with LATE estimates?
- Represent an aggregated effect that is hard to interpret directly

---
# Motivation: why aren't LATEs great already?

From Athey & Imbens, "The State of Applied Econometrics: Causality and Policy Evaluation" (2017): 

> "Even when a causal study is done carefully, both in analysis and design, there is often little assurance that the causal effects are valid for populations or settings other  than  those  studied.  "

---

# 4 Step Program

1. Bin - Partition the covariate space into subpopulations
2. Model - Construct a model for the variation in treatment effects across the population.
3. Project - Project our estimates onto a new population
4. Maximize - Maximize expected utility over the state-space generated by our model

---
# Background

Analyzing Treatment Effects in Subgroups:

- Hill - Bayesian Additive Regression Trees
- Athey and Imbens - Causal Trees
- Wager and Athey - Causal Forests
- Imai  and  Ratkovic - LASSO w/ Interactions

---
# Question 1:

As we construct a finer partition of the covariate space, the number of parameters to estimate grows geometrically. The variance of traditional estimators approaches infinity really fast! How can we estimate a rich set of variation without blowing up our estimates?

---
# Question 2:

Is sparsity a desirable property in causal estimation? What if our treatment effects are a smooth function of covariates? 

---
# Question 3:

How do we connect these estimates to policy?

---
# Proposal: Incorporate Information into Estimation

- We know things about distance in a covariate space
- We can incorporate that information in the form of a prior
- Methods for exploring the parameter space are efficient and statistically well understood
- Extrapolating and optimal policy is well understood: Posterior Predictive Distribution + vNM Utility

---
# Proposal: Incorporate Information into Estimation

Use a multilevel model, e.g. a latent likelihood for the way parameters relate to each other. Basic structure

Observational Model within each cell in the partition:
$$Y_i \sim f(X_i, \theta_i)$$

Likelihood ("prior") about how parameters relate across the partitions:

$$\theta_i \sim g(\Theta)$$

Prior on the hyper-parameters in order to penalize model complexity (e.g. regularization):

$$\Theta \sim \Pi(\Gamma)$$

---
# Proposal: Incorporate Information into Estimation
## Conditions

- Construct a prior that can recover any model of interest
  - Infinite variance of treatment effects
  - Constant variance of treatment effects
- Penalize complexity of the model such that ex-ante we don't expect huge parameter values
  - Regularization of hyperparameters
- Formulate models so that hyperparameters & priors are interpretable by humans
  
---
# Proposal: Incorporate Information into Estimation
## Specific Workable Proposal

- Think of given partition as a graph
- All ordered covariates are connected
- Covariates where we don't know order are disconnected
- Sidenote: this idea can co-exist with any of the other methods for partitioning the space (e.g. partitioning via forests first or the Finite Element Method as in spatial stats), but I'm going to work with even lattices for now

---
# Proposal: Incorporate Information into Estimation
## Specific Workable Proposal

- Borrow from the spatial literature: model treatment effects as varying according to a Gaussian Markov Random Field (GMRF) via Intrinsic Conditional Autoregressive Model (Random Walk prior)
- When including covariates w/o order, model variation across disconnected graphs via latent normal distribution (Besag, York, Mollie model)
- This is not the only model that might work in this framework, but I'll show that it works really nicely
- Other possibilities: gaussian process priors, non-random-walk (exact, stationary) autoregressive process, non-normal distributions for variation, probably lots of other things

---
# Heterogenous treatment effects via GMRFs
## What is a GMRF?

- Gaussian: normally distributed
- Markov: on depends on current state
- Random: varying over its dimensions stochastically
- Field: moves 1 or more dimensions

---
# Heterogenous treatment effects via GMRFs
## What is a GMRF?

What does it imply? Let's start with a 1 dimensional example, focusing on the blue dot,. Imagine we have broken a single covariate into a set of even bins like so:

```{r echo = F, message = F, warning = F, fig.height = 1}
library(ggplot2)

df <- data.frame(x = seq(0, 10, by = 1), y = rep(1, 11), color = c(rep(0, 5), 1, rep(0, 5)))

ggplot(df,
       aes(x = x, y = y, color = factor(color))) +
  geom_point(size = 2) +
  theme_void() +
  guides(color = F) +
  scale_color_manual(values = c("red", "blue"))
```

---
# Heterogenous treatment effects via GMRFs
## What is a GMRF?

The Gaussian Markov Random Field assumption implies that all of the prior observational knowledge about the blue dot is contained in its neighbors, the green dots.

```{r echo = F, message = F, warning = F, fig.height = 1}
df <- data.frame(x = seq(0, 10, by = 1), y = rep(1, 11), color = c(rep(0, 4),1, 2, 1, 
                                                                   rep(0, 4)))

ggplot(df,
       aes(x = x, y = y, color = factor(color))) +
  geom_point(size = 2) +
  theme_void() +
  guides(color = F) +
  scale_color_manual(values = c("red", "#66CD00", "blue"))
```

One way to think about this is via an autoregressive process (an AR(1) is a special case of a GMRF). You could select any number of neighbors, but I'm just using 1 in this paper. This is a simplifying assumption, but one that has a long history and strong mathematical connections to stochastic partial differential equations (see Lindgren, Rue, LindstrÃ¶m 2011).

---
# Heterogenous treatment effects via GMRFs
## What is a GMRF?

In other words, for some parameter of interest $phi$ and variance $\sigma$,

$p(\phi_{blue} | \phi_{color \neq blue}, \sigma) = p(\phi_{blue} | \phi_{green}, \sigma_i).$

```{r echo = F, message = F, warning = F, fig.height = 1}
df <- data.frame(x = seq(0, 10, by = 1), y = rep(1, 11), color = c(rep(0, 4),1, 2, 1, 
                                                                   rep(0, 4)))

ggplot(df,
       aes(x = x, y = y, color = factor(color))) +
  geom_point(size = 2) +
  theme_void() +
  guides(color = F) +
  scale_color_manual(values = c("red", "#66CD00", "blue"))
```

---
# Heterogenous treatment effects via GMRFs
## Conditional Autoregressive Processes

This model is normally an autoregressive one, parameterized by the precision (inverse of $\sigma$). So letting $i$ = blue,

$p(\phi_{i} | \phi_{j \neq i}, \sigma_i) = N(\mu_i, \sigma_i)$ where

$\mu_{blue} = \alpha\Sigma_{j \in green} w_{i,j}\phi_j$

and $w$ indicates the weight for the distance of $j$ from $i$.

---
# Heterogenous treatment effects via GMRFs
## Conditional Autoregressive Processes

This model is normally an autoregressive one, parameterized by the precision (inverse of $\sigma$). So

$p(\phi_{blue} | \phi_{color \neq blue}, \sigma_{blue}) = N(\mu_i, \sigma_{blue})$ where

$\mu_i = \alpha\Sigma_{j \in green} w_{i,j}\phi_j$

where $w$ indicates the weight for the distance of $j$ from $i$.

---
# Heterogenous treatment effects via GMRFs
## Intrinsic Conditional Autoregressive Processes

The "intrinsic" version of this model corresponds to a random walk, where $\alpha = 1$, a knife-edge case people are most familiar with in a time series context. 

For a detailed and practical exposition, refer to Morris et al. in _Spatial and spatio-temporal epidemiology_. I'm going to follow her notation.


---
# Heterogenous treatment effects via GMRFs
## Intrinsic Conditional Autoregressive Processes

Consider a lattice with $n$ cells. Let $w_{i,j} = 1$ if $i$, $j$ are neighbors, 0 otherwise. 

Let $D$ be an $n$ x $n$ diagonal matrix with $d_{i,i}$ indicating the number of neighbors a cell in the lattice has.

Let $B$ be a scaled weights matrix where the weights are normalized by the number of neighbors.

This corresponds to the assumption that _without seeing any data_, the mean of a cell is the average of its neighbors, with uncertainty decreasing with the number of neighbors it has. After some algebra,

$$\phi \sim N(0, [q(D - W)]^{-1})$$

where $q = \sigma^{-1}.$

---
# Heterogenous treatment effects via GMRFs

For many applications this seems like a sensible prior:

- We expect treatment effects to be a smooth function of covariates
- Possibility of non-linear movement in both treatment effects and outcomes
- Want to allow for potential non-stationarity of the process
- GMRF is basically a step-wise approximation via a lattice of this stuff!
- Allows us to identify models by sharing information across cells, instead of treating them as unrelated
- Models are identifiable even as the number of observations in a bin goes to 0

---
# Heterogenous treatment effects via GMRFs
## Estimation

We can estimate this model efficiently via Hamiltonian Monte Carlo, an MCMC method which gives us a good approximation the full posterior distribution for all parameters.

`Stan` is a probabilistic programming language that allows a nice declarative syntax for custom models, while implementing all this stuff on the back-end. For the code see my [github repository](www.github.com/be-green/late-to-great) in the `src` folder.

Bayesian inference for the parameters is straightforward, since the "latent" parameters are just like any other parameter in the model!

---
# Let's see some examples!

Computational experiment:

- Binary treatment
- Strong instrument
- Set of $k$ ordered covariates
- Set of $n$ groups per covariate
- This gives us $n^k$ groups total
- Treatment effect $\tau$ is a function of covariates
- Compare this model to independent estimation on the same partition

---
# Computational Experiments
## Understanding the setup

```{r eval = F}
# n = nobs
# k = ndim for covariate grid
# ngroups = number of groups per x variable, fine-ness of grid
n = 1000
k = 3
ngroups = 5 

squared <- function(x) x^2
all_fun <- c("I", "squared", "sin")

# construct random covariance matrix
s = tcrossprod(rnorm(k))
diag(s) = exp(rnorm(k)) + max(s)

# simulate covariates
X = MASS::mvrnorm(n = n, Sigma = s,
                  mu = rnorm(k, 0, 1))

```

---
# Computational Experiments
## Understanding the setup

```{r eval = F}
# betas are randomly drawn from a normal distribution
beta = rnorm(k, 0, 2)

# functions are sampled from function list
fun = sample(all_fun, 1)

# treatment effect
tau = as.vector(do.call(fun, args = list(X)) %*% beta)
tau = as.numeric(scale(tau))
```


---
# Computational Experiments
## Understanding the setup

```{r eval = F}
# People naturally take up the treatment if they like it enough
natural_uptake = fifelse(rank(tau)/length(tau) > 0.9, 1, 0)

# strong instrument that impacts assignment otherwise
instr = rnorm(n, mean = 0, sd = 10)
rand_assign = rbinom(n, 1, prob = plogis(instr))

# initially natural uptake, then people induced to treatment
actual_treatment = natural_uptake
actual_treatment[which(natural_uptake == 0)] <- 
  rand_assign[which(natural_uptake == 0)]

# actual outcomes
outcomes = tau * actual_treatment + rnorm(n)
```

---

```{r echo = F, message = F, warning = F, error = F, fig.width=9, dpi = 300}
library(data.table)
library(magrittr)
modcomp <- fread("model-comparisons/model-summary-stats.csv")
modcomp %>% 
  ggplot(aes(x = ngroups, y = MSE, color = Model)) + geom_point() + 
  geom_smooth(se = F) + 
  facet_wrap(~k, scales = 'free_y', ncol = 1, 
             labeller = function(x) lapply(x, function(x) paste0(x, " Covariate(s)"))) +
  scale_y_log10(labels = scales::comma) + 
  ggtitle("Mean Squared Error of Predicted Treatment Effect vs. True Group Average", 
          subtitle = "N = 1000, Treatment Effect Varies over Covariates")
```

---
```{r echo = F, message = F, warning = F, error = F, fig.width=9, dpi = 300}
modcomp %>% 
  ggplot(aes(x = ngroups, y = R2, color = Model)) + geom_point() + 
  geom_smooth(se = F) + 
  facet_wrap(~k, scales = 'free_y', ncol = 1, 
             labeller = function(x) lapply(x, function(x) paste0(x, " Covariate(s)"))) +
  scale_y_continuous(labels = scales::percent) + 
  ggtitle("R-squared of Predicted Treatment Effect vs. True Group Average", 
          subtitle = "N = 1000, Treatment Effect Varies over Covariates")
```

---

```{r echo = F, message = F, warning = F, error = F, fig.width=9, dpi = 300}
modcomp %>% 
  ggplot(aes(x = ngroups, y = bias, color = Model)) + geom_point() + 
  geom_smooth(se = F) + 
  facet_wrap(~k, scales = 'free_y', ncol = 1, 
             labeller = function(x) lapply(x, function(x) paste0(x, " Covariate(s)"))) +
  scale_y_continuous(labels = scales::comma) + 
  ggtitle("Bias of Predicted Treatment Effect vs. True Group Average", 
          subtitle = "N = 1000, Treatment Effect Varies over Covariates") +
  ylab("Bias")
```

---
# Experimental summary
### Why does this work so well?

- When there is no info, prior acts as a kind of basis function
- Let $d_{i,i}$ be the number of neighbors, $q_i^{-1}$ be the variance  of cell i, $i~j$ mean the neighbors of cell $i$
- The general case for the prior is

$p(\beta_i | \beta_j, j \neq i, q^{-1}) = N(\dfrac{\Sigma_{i\sim j}\beta_i}{d_{i,i}}, \dfrac{1}{d_{i,i}q_i})$

---
# Experimental summary
### Why does this work so well?

- Let's build some intuition!
- In our model, we have assumed a unit, quite noise process, so the prior is that $q_i^{-1} = \sigma_i = 1$
- Consider cell $i$, with estimates in $i+1$, $i-1$, in we were only in 1 dimension

$p(\beta_i) = p(\beta_i|\beta_{i+1}, \beta_{i-1}) = N\left(\dfrac{\beta_{i+1} + \beta_{i-1}}{2}, \dfrac{1}{2}\right)$
$E(\beta_i) = E \left(\dfrac{\beta_{i+1} + \beta_{i-1}}{2} \right)$

---
# Experimental summary
### Why does this work so well?

- If $\beta_{i + 1}$ and $\beta_{i - 1}$ were known, this would just be the average of the two, with the variance scaled by 1/2
- The prior is a linear basis function--we just drew a line between the two points
- Without any other identifying information we can use this to approximate a smooth function

---
# Experimental summary
### Why does this work so well?

- But wait, isn't that a bit overconfident?
- Remember, we don't know $\beta_{i-1}$ and $\beta_{i+1}$
- This corresponds to the distribution of all lines that are drawn between $\beta_{i-1}$ and $\beta_{i+1}$
- Weights of the distribution correspond to the likelihood at each point on $\beta_{i-1}$ and $\beta_{i+1}$


---
### Comparing Models: 1 Dimensions, 6 Groups, $\tau = \beta * X^2$

![](model-comparisons/n-1000,ngroups-6,k-1,fun-squared/posterior_interval_vs_raw.png)


---
### Comparing Models: 1 Dimensions, 6 Groups, $\tau = \beta * X^2$

![](model-comparisons/n-1000,ngroups-6,k-1,fun-squared/2sls_vs_mean.png)

---
### Comparing Models: 1 Dimensions, 6 Groups, $\tau = \beta * X^2$

![](model-comparisons/n-1000,ngroups-6,k-1,fun-squared/posterior_mean_vs_raw.png)

---
### Comparing Models: 3 Dimensions, 6 Groups, $\tau = \beta * X^2$

![](model-comparisons/n-1000,ngroups-6,k-3,fun-squared/posterior_interval_vs_raw.png)


---
### Comparing Models: 3 Dimensions, 6 Groups, $\tau = \beta * X^2$

![](model-comparisons/n-1000,ngroups-6,k-3,fun-squared/2sls_vs_mean.png)


---
### Comparing Models: 3 Dimensions, 6 Groups, $\tau = \beta * X^2$

![](model-comparisons/n-1000,ngroups-6,k-3,fun-squared/posterior_mean_vs_raw.png)












